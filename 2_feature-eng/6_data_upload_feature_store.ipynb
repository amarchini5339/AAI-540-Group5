{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2590d46b-f162-477c-be5e-dddaa3c5dd29",
   "metadata": {},
   "source": [
    "# Feature Store\n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e8fa701-4582-4c3f-af63-d742cf2ad4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install \"PyAthena[SQLAlchemy]\" sqlalchemy s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5aed7944-3dac-45f6-8458-62889b64541e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3, sagemaker, time\n",
    "import pandas as pd\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.feature_definition import FeatureDefinition, FeatureTypeEnum\n",
    "from sagemaker import get_execution_role\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "fg_name = \"aai540-ids-splitfs-v2-\" + strftime(\"%Y%m%d-%H%M%S\", gmtime())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb66d68-a8a0-4a63-adf6-6dbbd2b09cfb",
   "metadata": {},
   "source": [
    "## Fetch Sampled Data from Athena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d41a4801-87b9-4663-b4d4-f9035fb4d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 299782 rows for Feature Store ingestion.\n"
     ]
    }
   ],
   "source": [
    "database_name = \"aai540_eda\"\n",
    "athena_results_path = f\"s3://{bucket}/athena/staging/\"\n",
    "engine = create_engine(\n",
    "    f\"awsathena+rest://@athena.{region}.amazonaws.com:443/{database_name}\",\n",
    "    connect_args={\"s3_staging_dir\": athena_results_path, \"region_name\": region},\n",
    ")\n",
    "\n",
    "# pulling Train, Val, and Test splits\n",
    "query = f\"\"\"\n",
    "SELECT \n",
    "    *, \n",
    "    CAST(row_number() OVER() AS VARCHAR) AS record_id,\n",
    "    {time.time()} AS EventTime\n",
    "FROM {database_name}.split_v2\n",
    "WHERE data_split IN ('train', 'val', 'test')\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_sql(query, engine)\n",
    "print(f\"Loaded {len(df)} rows for Feature Store ingestion.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a21ed-49ab-4340-b22e-35b6a139c55e",
   "metadata": {},
   "source": [
    "## Define and Create the Feature Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60839839-2bd4-4843-ba3b-4be05d7125dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for Feature Group...\n",
      "Waiting for Feature Group...\n",
      "Waiting for Feature Group...\n",
      "Waiting for Feature Group...\n",
      "Feature Group Status: Created\n"
     ]
    }
   ],
   "source": [
    "feature_group = FeatureGroup(name=fg_name, sagemaker_session=sess)\n",
    "\n",
    "# map pandas dtypes to Feature Store types\n",
    "feature_group.load_feature_definitions(data_frame=df)\n",
    "\n",
    "# create with Online Store (for inference) and Offline Store (for training) enabled\n",
    "feature_group.create(\n",
    "    s3_uri=f\"s3://{bucket}/aai540/feature-store-offline/\",\n",
    "    record_identifier_name=\"record_id\",\n",
    "    event_time_feature_name=\"EventTime\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True\n",
    ")\n",
    "\n",
    "# wait for completion\n",
    "def wait_for_fg(fg):\n",
    "    status = fg.describe().get(\"FeatureGroupStatus\")\n",
    "    while status == \"Creating\":\n",
    "        print(\"Waiting for Feature Group...\")\n",
    "        time.sleep(5)\n",
    "        status = fg.describe().get(\"FeatureGroupStatus\")\n",
    "    print(f\"Feature Group Status: {status}\")\n",
    "\n",
    "wait_for_fg(feature_group)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa2a9e9-0c6c-449f-a6d0-6a7d90042210",
   "metadata": {},
   "source": [
    "## Ingest the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df680b79-2cf4-497e-a4cb-1abf5e473197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully ingested 299782 rows into aai540-ids-splitfs-v2-20260208-213935\n"
     ]
    }
   ],
   "source": [
    "# ingest using parallel workers\n",
    "feature_group.ingest(data_frame=df, max_workers=5, wait=True)\n",
    "\n",
    "print(f\"Successfully ingested {len(df)} rows into {fg_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0de93b-f31b-432d-b5d2-a089a3f721aa",
   "metadata": {},
   "source": [
    "## Export Production Holdout Set to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf40c742-e27d-4a21-b6c6-66c8e9580826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching production data from Athena...\n",
      "Successfully exported 1199728 production rows to:\n",
      "s3://sagemaker-us-east-1-128131109986/aai540/production_holdout/production_data.csv\n"
     ]
    }
   ],
   "source": [
    "# define the S3 path for the production holdout set\n",
    "prod_s3_path = f\"s3://{bucket}/aai540/production_holdout/production_data.csv\"\n",
    "\n",
    "# query to pull only the 'prod' split\n",
    "query_prod = f\"\"\"\n",
    "SELECT * FROM {database_name}.split_v1\n",
    "WHERE data_split = 'prod'\n",
    "\"\"\"\n",
    "\n",
    "print(\"Fetching production data from Athena...\")\n",
    "df_prod = pd.read_sql(query_prod, engine)\n",
    "\n",
    "# export to S3\n",
    "# index=False to keep the data clean for future inference\n",
    "df_prod.to_csv(prod_s3_path, index=False)\n",
    "\n",
    "print(f\"Successfully exported {len(df_prod)} production rows to:\")\n",
    "print(prod_s3_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
