{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccaf15d6-acf7-4d08-97b4-e82930064601",
   "metadata": {},
   "source": [
    "# MLOps CI/CD with SageMaker Pipelines\n",
    "\n",
    "This notebook implements a CI/CD workflow in SageMaker.\n",
    "\n",
    "## CI (Pipeline)\n",
    "1. Export train/val/test splits from SageMaker Feature Store (Offline Store)\n",
    "2. Run Hyperparameter Tuning on XGBoost (built-in container)\n",
    "3. Evaluate the best model on test set (AUC)\n",
    "4. Gate promotion based on AUC threshold\n",
    "5. Register the best model in Model Registry\n",
    "\n",
    "## CD\n",
    "- Deploy latest Approved model package to a fixed endpoint name.\n",
    "\n",
    "## Data format (XGBoost built-in)\n",
    "- CSV (no header)\n",
    "- Label is first column\n",
    "- All features numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23f35e-c856-4474-8fc2-be361df62d0f",
   "metadata": {},
   "source": [
    "## Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6786dca9-e530-48d8-b7ea-3a587c3feb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install sagemaker boto3 pandas \"PyAthena[SQLAlchemy]\" sqlalchemy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e42b4ec-83e2-4e18-93f5-32304d0e814d",
   "metadata": {},
   "source": [
    "## Setup AWS + SageMaker sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fe8cc2d-72a3-4ff7-8b81-e365848714c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "Region: us-east-1\n",
      "Bucket: sagemaker-us-east-1-128131109986\n",
      "Role  : arn:aws:iam::128131109986:role/LabRole\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "bucket = sess.default_bucket()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "sm = boto3.client(\"sagemaker\", region_name=region)\n",
    "s3 = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "print(\"Region:\", region)\n",
    "print(\"Bucket:\", bucket)\n",
    "print(\"Role  :\", role)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387ec4b9-7d28-43a0-bf3e-5d589142c1ca",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92b7a116-b294-4f45-9d4e-091a29ce85df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: aai540-ids-xgb-binary-cicd\n",
      "Model Package Group: aai540-ids-xgb-binary-pkg\n",
      "Endpoint: ids-xgboost-binary-cicd\n",
      "Min AUC: 0.85\n",
      "Feature Group: aai540-ids-splitfs-v2-20260213-052016\n"
     ]
    }
   ],
   "source": [
    "CICD_ENDPOINT_NAME = \"ids-xgboost-binary-cicd\"\n",
    "PIPELINE_NAME = \"aai540-ids-xgb-binary-cicd\"\n",
    "MODEL_PACKAGE_GROUP_NAME = \"aai540-ids-xgb-binary-pkg\"\n",
    "MIN_AUC_TO_DEPLOY_DEFAULT = 0.85\n",
    "\n",
    "fg_list = sm.list_feature_groups(NameContains='aai540-ids-splitfs', SortBy='CreationTime', SortOrder='Descending')\n",
    "FEATURE_GROUP_NAME = fg_list['FeatureGroupSummaries'][0]['FeatureGroupName']\n",
    "\n",
    "FEATURE_COLS = [\n",
    "    \"duration\",\"pkt_total\",\"bytes_total\",\n",
    "    \"pkt_fwd\",\"pkt_bwd\",\"bytes_fwd\",\"bytes_bwd\",\n",
    "    \"pkt_rate\",\"byte_rate\",\"bytes_per_pkt\",\n",
    "    \"pkt_ratio\",\"byte_ratio\",\n",
    "]\n",
    "LABEL_COL = \"label\"\n",
    "SPLIT_COL = \"data_split\"\n",
    "\n",
    "print(\"Pipeline:\", PIPELINE_NAME)\n",
    "print(\"Model Package Group:\", MODEL_PACKAGE_GROUP_NAME)\n",
    "print(\"Endpoint:\", CICD_ENDPOINT_NAME)\n",
    "print(\"Min AUC:\", MIN_AUC_TO_DEPLOY_DEFAULT)\n",
    "print(\"Feature Group:\", FEATURE_GROUP_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e199126-5e1b-438f-a195-65d045d2429c",
   "metadata": {},
   "source": [
    "## Load Feature Group and validate schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5544d12f-5a18-4c93-bbcc-a04c90bb92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Store schema validated\n"
     ]
    }
   ],
   "source": [
    "netdetect_fg = FeatureGroup(name=FEATURE_GROUP_NAME, sagemaker_session=sess)\n",
    "fg_desc = netdetect_fg.describe()\n",
    "\n",
    "fg_feature_names = {f[\"FeatureName\"] for f in fg_desc[\"FeatureDefinitions\"]}\n",
    "required = set([LABEL_COL, SPLIT_COL] + FEATURE_COLS)\n",
    "missing = required - fg_feature_names\n",
    "\n",
    "if missing:\n",
    "    raise ValueError(f\"Feature Store missing required columns: {sorted(list(missing))}\")\n",
    "\n",
    "print(\"Feature Store schema validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb4931f-74e2-4d33-8eaf-18fd4b611ae4",
   "metadata": {},
   "source": [
    "## Export train/val/test splits from Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1901c2a1-a6b1-4231-9123-3214f8319b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export run prefix: 20260221-020639\n",
      "Exporting splits...\n",
      "Done\n",
      "Train: s3://sagemaker-us-east-1-128131109986/aai540/cicd/exports/aai540-ids-xgb-binary-cicd/20260221-020639/train.csv (200,414 rows)\n",
      "Val: s3://sagemaker-us-east-1-128131109986/aai540/cicd/exports/aai540-ids-xgb-binary-cicd/20260221-020639/val.csv (50,100 rows)\n",
      "Test : s3://sagemaker-us-east-1-128131109986/aai540/cicd/exports/aai540-ids-xgb-binary-cicd/20260221-020639/test.csv (50,100 rows)\n"
     ]
    }
   ],
   "source": [
    "def export_split_from_feature_store_to_s3(split_name: str, run_prefix: str):\n",
    "    query = netdetect_fg.athena_query()\n",
    "\n",
    "    # force numeric casts (avoids XGBoost failures due to strings/null types)\n",
    "    feature_expr = \", \".join([f\"CAST({c} AS DOUBLE) AS {c}\" for c in FEATURE_COLS])\n",
    "\n",
    "    query_string = f\"\"\"\n",
    "    SELECT\n",
    "      CAST({LABEL_COL} AS DOUBLE) AS {LABEL_COL},\n",
    "      {feature_expr}\n",
    "    FROM \"{query.table_name}\"\n",
    "    WHERE {SPLIT_COL} = '{split_name}'\n",
    "    \"\"\"\n",
    "\n",
    "    # athena intermediate output\n",
    "    athena_output = f\"s3://{bucket}/aai540/athena_exports/{run_prefix}/\"\n",
    "    query.run(query_string=query_string, output_location=athena_output)\n",
    "    query.wait()\n",
    "\n",
    "    # consolidate to single CSV\n",
    "    df = query.as_dataframe()\n",
    "    local_path = f\"/tmp/{split_name}.csv\"\n",
    "    df.to_csv(local_path, header=False, index=False)\n",
    "\n",
    "    key = f\"aai540/cicd/exports/{PIPELINE_NAME}/{run_prefix}/{split_name}.csv\"\n",
    "    s3.upload_file(local_path, bucket, key)\n",
    "\n",
    "    return f\"s3://{bucket}/{key}\", len(df)\n",
    "\n",
    "run_prefix = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "print(\"Export run prefix:\", run_prefix)\n",
    "\n",
    "print(\"Exporting splits...\")\n",
    "train_s3, n_train = export_split_from_feature_store_to_s3(\"train\", run_prefix)\n",
    "val_s3, n_val   = export_split_from_feature_store_to_s3(\"val\", run_prefix)\n",
    "test_s3, n_test  = export_split_from_feature_store_to_s3(\"test\", run_prefix)\n",
    "print(\"Done\")\n",
    "\n",
    "print(\"Train:\", train_s3, f\"({n_train:,} rows)\")\n",
    "print(\"Val:\", val_s3, f\"({n_val:,} rows)\")\n",
    "print(\"Test :\", test_s3, f\"({n_test:,} rows)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378c7b8-2c06-4d4a-8574-fd0e95f4bda8",
   "metadata": {},
   "source": [
    "## Preflight checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a96da1d-3f1f-4fa7-a362-12bf42650907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 exists train: True\n",
      "S3 exists val  : True\n",
      "S3 exists test : True\n",
      "Train sample shape: (50, 13)\n",
      "Label uniques (sample): [0.0, 1.0]\n",
      "Any nulls (sample): False\n"
     ]
    }
   ],
   "source": [
    "from urllib.parse import urlparse\n",
    "\n",
    "def head_s3(uri: str):\n",
    "    u = urlparse(uri)\n",
    "    b = u.netloc\n",
    "    k = u.path.lstrip(\"/\")\n",
    "    s3.head_object(Bucket=b, Key=k)\n",
    "    return True\n",
    "\n",
    "print(\"S3 exists train:\", head_s3(train_s3))\n",
    "print(\"S3 exists val  :\", head_s3(val_s3))\n",
    "print(\"S3 exists test :\", head_s3(test_s3))\n",
    "\n",
    "df_chk = pd.read_csv(\"/tmp/train.csv\", header=None, nrows=50)\n",
    "print(\"Train sample shape:\", df_chk.shape)\n",
    "print(\"Label uniques (sample):\", sorted(df_chk[0].unique())[:10])\n",
    "print(\"Any nulls (sample):\", df_chk.isna().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5226c3d0-f4cd-4dab-9725-14c2f98fe7cd",
   "metadata": {},
   "source": [
    "## Write evaluation script (AUC) with robust tar extraction\n",
    "\n",
    "This script:\n",
    "- dynamically installs xgboost into the sklearn container\n",
    "- extracts any .tar.gz in the model directory\n",
    "- finds the XGBoost model file\n",
    "- evaluates AUC on test.csv\n",
    "- writes evaluation.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f05fea3e-9481-4b32-a6eb-c21eea1f2b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote evaluate.py\n"
     ]
    }
   ],
   "source": [
    "EVAL_SCRIPT = r\"\"\"\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# 1. Install xgboost into the sklearn container dynamically\n",
    "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"xgboost\"])\n",
    "\n",
    "# 2. Now perform our imports\n",
    "import json\n",
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import xgboost as xgb\n",
    "\n",
    "def main():\n",
    "    model_dir = \"/opt/ml/processing/model\"\n",
    "    test_path = \"/opt/ml/processing/test/test.csv\"\n",
    "    out_path  = \"/opt/ml/processing/evaluation/evaluation.json\"\n",
    "\n",
    "    # Extract any tar.gz present\n",
    "    for fname in os.listdir(model_dir):\n",
    "        if fname.endswith(\".tar.gz\"):\n",
    "            tar_path = os.path.join(model_dir, fname)\n",
    "            with tarfile.open(tar_path, \"r:gz\") as tar:\n",
    "                tar.extractall(path=model_dir)\n",
    "\n",
    "    # Prefer standard filenames\n",
    "    candidates = [\"xgboost-model\", \"model.xgb\", \"model\"]\n",
    "    model_path = None\n",
    "    for c in candidates:\n",
    "        p = os.path.join(model_dir, c)\n",
    "        if os.path.exists(p) and os.path.isfile(p):\n",
    "            model_path = p\n",
    "            break\n",
    "\n",
    "    # Last resort: pick first file that isn't tar.gz\n",
    "    if model_path is None:\n",
    "        for fname in os.listdir(model_dir):\n",
    "            p = os.path.join(model_dir, fname)\n",
    "            if os.path.isfile(p) and not fname.endswith(\".tar.gz\"):\n",
    "                model_path = p\n",
    "                break\n",
    "\n",
    "    if model_path is None:\n",
    "        raise FileNotFoundError(f\"No model file found. Contents: {os.listdir(model_dir)}\")\n",
    "\n",
    "    df = pd.read_csv(test_path, header=None)\n",
    "    y = df.iloc[:, 0].values\n",
    "    X = df.iloc[:, 1:].values\n",
    "\n",
    "    booster = xgb.Booster()\n",
    "    booster.load_model(model_path)\n",
    "\n",
    "    preds = booster.predict(xgb.DMatrix(X))\n",
    "    auc = float(roc_auc_score(y, preds))\n",
    "\n",
    "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "    with open(out_path, \"w\") as f:\n",
    "        json.dump({\"binary_classification_metrics\": {\"auc\": {\"value\": auc}}}, f)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\"\"\"\n",
    "with open(\"evaluate.py\", \"w\") as f:\n",
    "    f.write(EVAL_SCRIPT)\n",
    "\n",
    "print(\"Wrote evaluate.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d5ec1d-6376-4567-9e50-fab0d5017b87",
   "metadata": {},
   "source": [
    "## Pipeline imports (SageMaker Pipelines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4bc84dd-7a53-4c7d-8f48-10c8b8303202",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.pipeline_context import PipelineSession\n",
    "from sagemaker.workflow.parameters import ParameterFloat, ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TuningStep\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.functions import JsonGet\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.processing import ScriptProcessor, ProcessingInput, ProcessingOutput\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d525e9-d71c-4916-8e84-1d3f35d2dd06",
   "metadata": {},
   "source": [
    "## Define pipeline parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2256ccb1-df5a-4874-b58a-5978ac828f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_session = PipelineSession()\n",
    "\n",
    "min_auc_param = ParameterFloat(name=\"MinAUCToDeploy\", default_value=MIN_AUC_TO_DEPLOY_DEFAULT)\n",
    "train_s3_param = ParameterString(name=\"TrainDataS3Uri\", default_value=train_s3)\n",
    "val_s3_param   = ParameterString(name=\"ValDataS3Uri\",   default_value=val_s3)\n",
    "test_s3_param  = ParameterString(name=\"TestDataS3Uri\",  default_value=test_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622cd423-7025-40b8-8952-a11ebb7da9bd",
   "metadata": {},
   "source": [
    "## Define XGBoost estimator (built-in container) + set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6bdf1b1-e64c-40b9-a945-b2faffa41820",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n"
     ]
    }
   ],
   "source": [
    "xgb_image = image_uris.retrieve(\n",
    "    framework=\"xgboost\",\n",
    "    region=region,\n",
    "    version=\"1.7-1\"\n",
    ")\n",
    "\n",
    "xgb = Estimator(\n",
    "    image_uri=xgb_image,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=f\"s3://{bucket}/aai540/cicd/model_artifacts/{PIPELINE_NAME}/\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "# Static hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"auc\",\n",
    "    num_round=150,\n",
    "    scale_pos_weight=0.3\n",
    ")\n",
    "\n",
    "# Hyperparameter ranges\n",
    "hyperparameter_ranges = {\n",
    "    \"max_depth\":        IntegerParameter(1, 6),\n",
    "    \"eta\":              ContinuousParameter(0.01, 0.1, scaling_type=\"Logarithmic\"),\n",
    "    \"min_child_weight\": IntegerParameter(1, 10),\n",
    "    \"subsample\":        ContinuousParameter(0.5, 1.0),\n",
    "    \"gamma\":            ContinuousParameter(0.0, 5.0),\n",
    "}\n",
    "\n",
    "# Define the Tuner\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=xgb,\n",
    "    objective_metric_name=\"validation:auc\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    max_jobs=5,\n",
    "    max_parallel_jobs=2,\n",
    "    strategy=\"Bayesian\",\n",
    "    objective_type=\"Maximize\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ebb106-9ee1-4ec0-8dd1-12a43deeaca7",
   "metadata": {},
   "source": [
    "## TuningStep and Best Model Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1d130dfe-341c-44c2-98f5-9dd12cd5e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuning_step = TuningStep(\n",
    "    name=\"TuneXGB\",\n",
    "    tuner=tuner,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(s3_data=train_s3_param, content_type=\"text/csv\"),\n",
    "        \"validation\": TrainingInput(s3_data=val_s3_param, content_type=\"text/csv\"),\n",
    "    },\n",
    ")\n",
    "\n",
    "# Grab the S3 URI of the best model to pass to downstream steps\n",
    "top_model_uri = tuning_step.get_top_model_s3_uri(\n",
    "    top_k=0, \n",
    "    s3_bucket=bucket, \n",
    "    prefix=f\"aai540/cicd/model_artifacts/{PIPELINE_NAME}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c996c35-5da8-45f7-b4cb-6f6991389ec0",
   "metadata": {},
   "source": [
    "## Evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d0aafa3-4c1d-4d52-887d-6c9e5d1ad9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Defaulting to only available Python version: py3\n",
      "INFO:sagemaker.image_uris:Defaulting to only supported image scope: cpu.\n"
     ]
    }
   ],
   "source": [
    "script_processor = ScriptProcessor(\n",
    "    image_uri=image_uris.retrieve(\"sklearn\", region=region, version=\"1.2-1\"),\n",
    "    command=[\"python3\"],\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "evaluation_report = PropertyFile(\n",
    "    name=\"EvaluationReport\",\n",
    "    output_name=\"evaluation\",\n",
    "    path=\"evaluation.json\",\n",
    ")\n",
    "\n",
    "eval_step = ProcessingStep(\n",
    "    name=\"EvaluateModel\",\n",
    "    processor=script_processor,\n",
    "    code=\"evaluate.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=top_model_uri,\n",
    "            destination=\"/opt/ml/processing/model\",\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=test_s3_param,\n",
    "            destination=\"/opt/ml/processing/test\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"evaluation\",\n",
    "            source=\"/opt/ml/processing/evaluation\",\n",
    "        ),\n",
    "    ],\n",
    "    property_files=[evaluation_report],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce86b48b-0ce9-4e78-97ea-f58ce4c37dfe",
   "metadata": {},
   "source": [
    "## Gate on AUC + Register model in Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f42b0cbc-3819-45f3-bf50-50a43cb027a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_value = JsonGet(\n",
    "    step_name=eval_step.name,\n",
    "    property_file=evaluation_report,\n",
    "    json_path=\"binary_classification_metrics.auc.value\",\n",
    ")\n",
    "\n",
    "condition = ConditionGreaterThanOrEqualTo(\n",
    "    left=auc_value,\n",
    "    right=min_auc_param,\n",
    ")\n",
    "\n",
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=xgb,\n",
    "    model_data=top_model_uri,\n",
    "    model_package_group_name=MODEL_PACKAGE_GROUP_NAME,\n",
    "    approval_status=\"Approved\",\n",
    "    content_types=[\"text/csv\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.m5.large\"],\n",
    "    transform_instances=[\"ml.m5.large\"],\n",
    ")\n",
    "\n",
    "cond_step = ConditionStep(\n",
    "    name=\"AUCGate\",\n",
    "    conditions=[condition],\n",
    "    if_steps=[register_step],\n",
    "    else_steps=[],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d166f5-c89b-40c7-8f57-28792fdf76e6",
   "metadata": {},
   "source": [
    "## Build + upsert pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2844a62d-114f-44e7-9e48-9aacbbf290d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'HyperParameterTuningJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ProcessingJobName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n",
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "WARNING:sagemaker.workflow._utils:Popping out 'CertifyForMarketplace' from the pipeline definition since it will be overridden in pipeline execution time.\n",
      "WARNING:sagemaker.workflow.utilities:Popping out 'ModelPackageName' from the pipeline definition by default since it will be overridden at pipeline execution time. Please utilize the PipelineDefinitionConfig to persist this field in the pipeline definition if desired.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline upserted aai540-ids-xgb-binary-cicd\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=PIPELINE_NAME,\n",
    "    parameters=[min_auc_param, train_s3_param, val_s3_param, test_s3_param],\n",
    "    steps=[tuning_step, eval_step, cond_step],\n",
    "    sagemaker_session=pipeline_session,\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "print(\"Pipeline upserted\", PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90142d6-f311-49a3-812d-9e0a21f76c38",
   "metadata": {},
   "source": [
    "## Run pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25cf5b9f-08fd-44c0-9a83-e3548c5f5554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline started: arn:aws:sagemaker:us-east-1:128131109986:pipeline/aai540-ids-xgb-binary-cicd/execution/d0dauqf0adwa\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Executing\n",
      "Status: Succeeded\n",
      "\n",
      "Final status: Succeeded\n",
      "\n",
      "Step statuses:\n",
      "- RegisterModel-RegisterModel : Succeeded\n",
      "- AUCGate : Succeeded\n",
      "- EvaluateModel : Succeeded\n",
      "- TuneXGB : Succeeded\n"
     ]
    }
   ],
   "source": [
    "execution = pipeline.start(\n",
    "    parameters={\n",
    "        \"MinAUCToDeploy\": MIN_AUC_TO_DEPLOY_DEFAULT,\n",
    "        \"TrainDataS3Uri\": train_s3,\n",
    "        \"ValDataS3Uri\": val_s3,\n",
    "        \"TestDataS3Uri\": test_s3,\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"Pipeline started:\", execution.arn)\n",
    "\n",
    "while True:\n",
    "    desc = execution.describe()\n",
    "    status = desc[\"PipelineExecutionStatus\"]\n",
    "    print(\"Status:\", status)\n",
    "    if status in (\"Succeeded\", \"Failed\", \"Stopped\"):\n",
    "        break\n",
    "    time.sleep(30)\n",
    "\n",
    "print(\"\\nFinal status:\", status)\n",
    "if desc.get(\"FailureReason\"):\n",
    "    print(\"\\nFailureReason:\\n\", desc[\"FailureReason\"])\n",
    "\n",
    "print(\"\\nStep statuses:\")\n",
    "for s in execution.list_steps():\n",
    "    print(\"-\", s.get(\"StepName\"), \":\", s.get(\"StepStatus\"))\n",
    "    if s.get(\"StepStatus\") in (\"Failed\", \"Stopped\"):\n",
    "        print(\"  FailureReason:\", s.get(\"FailureReason\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645d2647-96b0-4724-b4c2-dc41ced30090",
   "metadata": {},
   "source": [
    "## CD: Deploy latest Approved model package to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc27a10a-553f-46da-8a43-dedeec1be8b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: aai540-ids-xgb-binary-pkg-2026-02-21-02-54-08-492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest Approved ModelPackage ARN: arn:aws:sagemaker:us-east-1:128131109986:model-package/aai540-ids-xgb-binary-pkg/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name ids-xgboost-binary-cicd\n",
      "INFO:sagemaker:Creating endpoint with name ids-xgboost-binary-cicd\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------!Deployed endpoint: ids-xgboost-binary-cicd\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import ModelPackage \n",
    "\n",
    "def get_latest_approved_model_package(group_name: str):\n",
    "    resp = sm.list_model_packages(\n",
    "        ModelPackageGroupName=group_name,\n",
    "        ModelApprovalStatus=\"Approved\",\n",
    "        SortBy=\"CreationTime\",\n",
    "        SortOrder=\"Descending\",\n",
    "        MaxResults=1,\n",
    "    )\n",
    "    pkgs = resp.get(\"ModelPackageSummaryList\", [])\n",
    "    if not pkgs:\n",
    "        raise RuntimeError(f\"No Approved model packages found in group: {group_name}\")\n",
    "    return pkgs[0][\"ModelPackageArn\"]\n",
    "\n",
    "latest_pkg_arn = get_latest_approved_model_package(MODEL_PACKAGE_GROUP_NAME)\n",
    "print(\"Latest Approved ModelPackage ARN:\", latest_pkg_arn)\n",
    "\n",
    "mp = ModelPackage(\n",
    "    role=role,\n",
    "    model_package_arn=latest_pkg_arn,\n",
    "    sagemaker_session=sess,\n",
    ")\n",
    "\n",
    "predictor = mp.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    endpoint_name=CICD_ENDPOINT_NAME,\n",
    ")\n",
    "\n",
    "print(\"Deployed endpoint:\", CICD_ENDPOINT_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
